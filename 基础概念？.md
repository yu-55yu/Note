1. Python有“解释器”和“脚本文件”两种运行模式

2. 在计算机视觉领域，常用的特征量包括 SIFT、SURF和HOG等。使用这些特征量将图像数据转换为向量，然后对 转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。

   ![image-20251017004437300](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251017004437300.png)

3. 交叉熵误差

   ![image-20251017004747204](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251017004747204.png)

   ![image-20251017004800351](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251017004800351.png)

   ，log表示以e为底数的自然对数（loge）。 yk是神经网络的输出，tk是 正确解标签。并且，tk中只有正确解标签的索引为1，其他均为0（one-hot表 示 ）。 因此，式（4.2）实际上只计算对应正确解标签的输出的自然对数。比如，假设 正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差 是−log0.6 =0.51； 若“2”对应的输出是0.1，则交叉熵误差为−log0.1=2.30。 也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的

4. one-hot表示。

5. ![image-20251017020154189](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251017020154189.png)

6. Lasagne深度学习框架

7. Embedding 是一种将“事物”表示为“向量”的方法

8. patch：指从一张大图像中分割出来的**一小块矩形区域**（图像块）。

   **应用**：

   - 在**卷积神经网络 (CNN)** 中，卷积核（filter）滑过图像时，它所“看到”的局部区域就可以被理解为一个 "patch"。
   - 在 **Vision Transformer (ViT)** 模型中，这个概念尤其重要。ViT 会先把一张完整的大图切割成 N 个固定大小的 "patches"（比如 16x16 像素的小方块），然后把这些 "patches" 序列化（拉平成向量），再像处理文本一样（类似 Transformer 处理单词）输入到模型中。


![image-20251018225858794](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251018225858794.png)

1. U-Net 全解析：从网络架构、核心原理到 PyTorch 代码实现

   [1]: https://blog.csdn.net/AggressiveYu/article/details/151256739	"U-Net"

2. DDPM

   通过给定的采样x，学习建模数据中真实分布px

   ![image-20251014170723552](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251014170723552.png)

   目标：最小化KL散度

   整个公式 `q(xₜ|xₜ₋₁) := N(xₜ; √1−βₜ * xₜ₋₁, βₜI)` 的完整含义是：

   > **“从状态 `xₜ₋₁` 到 `xₜ` 的转移规则 `q` 被定义为一个正态分布 `N`。这个分布的中心（均值）是 `xₜ₋₁` 乘以一个缩小因子 `√1−βₜ`，并且在这个基础上再增加一个强度为 `βₜ` 的随机高斯噪声。”**

3. DiT (Diffusion Transformer) - 扩散Transformer

4. 对抗学习”（Adversarial Learning）是一种通过**让两个神经网络相互博弈**来学习的方法。

   这种学习方法的核心是**生成对抗网络 (GAN)**，由一个生成器和一个判别器组成：

   - **生成器 (Generator)**：它负责创建新的数据样本，比如图像。
   - **判别器 (Discriminator)**：它的任务是区分生成器创建的假样本和真实的数据样本。

   在训练过程中，这两个网络相互竞争，不断提高各自的能力。生成器努力创建越来越逼真的假数据，以骗过判别器。同时，判别器则努力提高自己的识别能力，以便更好地分辨真假数据。这种竞争性的训练过程最终会使生成器能够产生高质量的合成数据，这些数据可以用于数据增强等场景。

5. **Embedding** 是一种将非数字数据（比如文本、图像、或用户ID）转换成**数字向量**的方法。这个向量被称为**嵌入向量（embedding vector）**。

6. **（5k epochs; ~5 分钟）**: 这两个数字是模型的**训练参数**和**耗时**。

   - **5k epochs**: 指的是模型进行了 **5000 个训练周期**（epochs）。一个 epoch 意味着模型遍历了整个训练数据集一次。
   - **~5 分钟**: 表示完成这 5000 个训练周期，整个训练过程大约花费了 **5 分钟**。

   **DIV2K/test/00.png**: 这部分指定了**用于演示的输入数据**。

   - **DIV2K**: 是一个知名的**高质量图像数据集**，常用于图像超分辨率等计算机视觉任务。
   - **test/00.png**: 这表示使用的是 **DIV2K 数据集中的测试集**下的一个具体图片文件，文件名为 `00.png`。

   文件夹 (Folders)

   - **`assets`**: 这个文件夹通常用来存放项目所使用的静态资源，比如图片、视频、演示文稿或者其他非代码文件。
   - **`components`**: 如果项目被模块化设计，这个文件夹会包含可重用的组件或功能模块。
   - **`data`**: 顾名思义，这里是存放项目所需数据的目录。这可能包括原始数据集、处理后的数据集或任何其他输入数据。
   - **`trainer`**: 这个文件夹通常用于存放训练模型的代码。比如，训练循环、损失函数计算或优化器配置等。
   - **`util`** (或者 `utils`): `util` 是 "utilities" 的缩写，表示工具函数。这个文件夹里通常是项目中的各种辅助脚本和通用函数，比如数据加载、文件处理或日志记录等。

   文件 (Files)           

   - **`.gitignore`**: 这是一个版本控制系统 **Git** 的配置文件。它告诉 Git 哪些文件或文件夹应该被忽略，不被添加到版本库中，比如编译生成的文件、临时文件或敏感配置信息。
   - **`LICENSE`**: 许可证文件。它定义了其他人如何可以合法地使用、修改和分发这个项目。
   - **`README.md`**: 项目的入口文件。通常是项目的说明书，会包含项目的介绍、安装指南、使用方法、依赖要求和联系信息等重要内容。
   - **`demo.py`**: 用于演示项目功能的脚本。通常运行这个文件可以看到一个完整的示例，展示项目的核心功能和效果。
   - **`main.py`** 或 **`run.py`**: 这两个文件通常是项目的**主执行脚本**。运行它们可以启动整个程序，比如开始训练或运行推理。
   - **`manager.py`**: 这是一个管理脚本，可能用来协调和管理项目中的各种任务，比如模型的保存与加载、训练的开始与停止等。
   - **`models.py`**: 这个文件会定义项目的模型架构，比如神经网络的层、结构和前向传播逻辑等。
   - **`opt.py`**: 经常是 "options" 或 "optimizer" 的缩写。它可能包含命令行参数配置，或者用于定义训练的超参数、优化器设置等。

7. ![transformer](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251015040748370.png)

8. 无论输入是图片还是视频，它们最终都会被转换成一个 **三维** 的张量 (Tensor) 输入到 Transformer 编码器中，这个形状是：$$(B, L, D)$$

   - **$B$ (Batch Size):** 批量大小，表示一次处理多少个样本（图片或视频）。
   - **$L$ (Sequence Length):** 序列长度，表示每个样本被拆分成了多少个“块”(Token)。
   - **$D$ (Embedding Dimension):** 嵌入维度，表示用来代表每一个“块”的向量的长度。

   ------

   下面我们以最常见的 Vision Transformer (ViT) 为例，详细分解 Shape 的变化过程：

   1. **图片 Vision Transformer, ViT)**

   我们假设使用一个常见的配置：

   - 输入图片尺寸 $(H, W, C)$: `(224, 224, 3)` (高=224, 宽=224, 通道=3)
   - 图像块大小 $(P, P)$: `(16, 16)`
   - 批量大小 $B$: `1` (为了简化说明)
   - 嵌入维度 $D$: `768`

   **Shape 变化流程：**

   1. **原始输入 (Input Image):**
      - Shape: `(B, C, H, W)`
      - *示例:* `(1, 3, 224, 224)`
   2. **分块与线性映射 (Patching + Projection):**
      - 这一步通常用一个卷积层 (Conv2D) 高效实现。这个卷积层的**步长 (Stride)** 和**卷积核 (Kernel)** 大小都等于图像块大小 $P$。
      - 卷积核大小: `(16, 16)`
      - 步长: `(16, 16)`
      - 输出通道数: `D` (即 768)
      - 卷积操作会输出一个新的特征图，其 Shape 为: `(B, D, H/P, W/P)`
      - *示例:* `(1, 768, 224/16, 224/16)` $\rightarrow$ `(1, 768, 14, 14)`
   3. **展平 (Flatten):**
      - Transformer 需要一个 1D 序列。我们将后面两个空间维度 (`14, 14`) 展平。
      - 首先计算序列长度 $L_{patches} = (H/P) \times (W/P) = 14 \times 14 = 196$。
      - Shape 变为: `(B, D, L_patches)`
      - *示例:* `(1, 768, 196)`
      - 为了符合 Transformer 的标准输入格式，通常会调整一下维度顺序：
      - Shape: `(B, L_patches, D)`
      - *示例:* `(1, 196, 768)`
   4. **添加 [CLS] Token:**
      - 在序列的开头加入一个可学习的 `[CLS]` 标记（用于最终分类）。
      - `[CLS]` Token 的 Shape: `(B, 1, D)`
      - 拼接 (Concatenate) 后的 Shape: `(B, L_patches + 1, D)`
      - *示例:* `(1, 196 + 1, 768)` $\rightarrow$ `(1, 197, 768)`
      - 此时，总序列长度 $L = 197$。
   5. **添加位置编码 (Positional Embedding):**
      - 位置编码是一个可学习的张量，它的 Shape 和上一步完全相同: `(1, L, D)` 或 `(B, L, D)`。
      - 它通过**加法**合并到序列中，所以**不会改变 Shape**。
      - *示例:* `(1, 197, 768) + (1, 197, 768)` $\rightarrow$ `(1, 197, 768)`

   最终输入到 Transformer 的 Shape: (1, 197, 768)

   即 (B, L, D)，其中 L=(PH×PW)+1。

   **2. 视频 (Video Vision Transformer, ViViT)**

   视频多了一个**时间维度 $T$ (帧数)**。我们以 "Tubelet" 嵌入法为例：

   - 输入视频尺寸 $(T, H, W, C)$: `(16, 224, 224, 3)` (16 帧)
   - Tubelet 块大小 $(t, h, w)$: `(2, 16, 16)` (时间深度=2, 高=16, 宽=16)
   - 批量大小 $B$: `1`
   - 嵌入维度 $D$: `768`

   **Shape 变化流程：**

   1. **原始输入 (Input Video):**
      - Shape: `(B, C, T, H, W)`
      - *示例:* `(1, 3, 16, 224, 224)`
   2. **3D分块与映射 (Tubelet Embedding):**
      - 使用一个 3D 卷积 (Conv3D) 实现。
      - 卷积核大小: `(t, h, w)` $\rightarrow$ `(2, 16, 16)`
      - 步长: `(t, h, w)` $\rightarrow$ `(2, 16, 16)`
      - 输出通道数: `D` (即 768)
      - 卷积输出 Shape: `(B, D, T/t, H/h, W/w)`
      - *示例:* `(1, 768, 16/2, 224/16, 224/16)` $\rightarrow$ `(1, 768, 8, 14, 14)`
   3. **展平 (Flatten):**
      - 将所有时空维度 (`8, 14, 14`) 展平。
      - 序列长度 $L_{tubelets} = (T/t) \times (H/h) \times (W/w) = 8 \times 14 \times 14 = 1568$。
      - Shape 变为: `(B, D, L_tubelets)` $\rightarrow$ `(1, 768, 1568)`
      - 调整维度顺序: `(B, L_tubelets, D)` $\rightarrow$ `(1, 1568, 768)`
   4. **添加 [CLS] Token:**
      - 与图片类似，添加 `[CLS]` 标记。
      - Shape: `(B, L_tubelets + 1, D)`
      - *示例:* `(1, 1568 + 1, 768)` $\rightarrow$ `(1, 1569, 768)`
      - 总序列长度 $L = 1569$。
   5. **添加位置编码 (Positional Embedding):**
      - 添加 Shape 为 `(1, 1569, 768)` 的位置编码，Shape 不变。

   最终输入到 Transformer 的 Shape: (1, 1569, 768)

   即 (B, L, D)，其中 L=(tT×hH×wW)+1。

   

9. 图片和视频输入到 Transformer 模型（如 Vision Transformer, ViT）时，并不是直接输入原始的像素数据，而是需要先将它们转换成一种类似文本处理中的 “词向量”（Token Embeddings）的序列。

   这个过程的核心是把图像或视频“切块”并“向量化”。

   1. 图片输入 (Vision Transformer, ViT)

   对于单张图片，输入的是一系列的 **“图像块嵌入 (Patch Embeddings)”**。

   过程如下：

   1. **分块 (Patching):**
      - 将一张完整的图片（例如 224x224 像素）切割成 $N$ 个固定大小的小图像块（Patches）。
      - 例如，使用 16x16 大小的图像块，一张 224x224 的图片就会被切成 $(224/16) \times (224/16) = 14 \times 14 = 196$ 个小块。
   2. **展平 (Flatten):**
      - 将每个小图像块（Patch）展平（Flatten）成一个一维向量。
      - 如果一个图像块是 16x16 像素，并且是彩色的（RGB, 3个通道），那么展平后的向量维度就是 $16 \times 16 \times 3 = 768$。
   3. **线性映射 (Linear Projection):**
      - 将这个展平的 768 维向量通过一个线性层（Linear Layer）映射（或称为“嵌入”，Embedding）到一个更高维度的向量空间（例如 1024 维）。这个输出的向量就是该图像块的 **Patch Embedding**。
   4. **加入位置编码 (Positional Embedding):**
      - 标准的 Transformer 是“序列无关”的，它不知道每个输入元素的顺序。但图像块的位置信息至关重要（哪个块在左上角，哪个在中间）。
      - 因此，模型会为每个 Patch Embedding 添加一个可学习的 **“位置编码”** 向量，告诉模型这个图像块来自原始图像的哪个位置。
   5. **加入[CLS]标记 (CLS Token):**
      - 在序列的最前面，会添加一个特殊的、可学习的 `[CLS]` (Classification) 标记。这个标记的最终输出向量将被用来代表整个图像进行分类。

   总结一下，输入给 Transformer 的是一组向量序列，包括：

   [ [CLS]标记 + [位置编码], [图像块1的嵌入] + [位置编码1], [图像块2的嵌入] + [位置编码2], ... ]

   2. 视频输入 (Video Vision Transformer, ViViT)

   

   对于视频，模型不仅要理解每帧图像的空间内容，还要理解帧与帧之间的时间关系。因此，输入的是 **“时空块嵌入 (Spatio-temporal Embeddings)”**。

   主要有两种主流方法：

   方法一：Tubelet 嵌入 (Tubelet Embedding)

   这是最直接的方法，把“分块”从 2D 扩展到 3D。

   1. **3D分块 (Tubelet):**
      - 不再是切割 2D 的图像块，而是从视频中切割 3D 的 **“管块” (Tubelets)**。
      - 一个 Tubelet 不仅有高度和宽度（例如 16x16 像素），还有时间深度（例如 2 帧）。
      - 例如，一个 $2 \times 16 \times 16$ 的 Tubelet 捕获了 2 帧图像在同一空间位置的信息。
   2. **展平与映射:**
      - 将这个 3D 的 Tubelet 展平并进行线性映射，得到一个代表了“时空”片段的嵌入向量。
   3. **加入时空位置编码:**
      - 位置编码也需要是 3D 的，需要同时告诉模型这个 Tubelet 在空间上（帧内的 H, W 位置）和时间上（视频的第几帧）的位置。

   方法二：时空分解 (Factorized Encoder)

   这种方法将空间和时间分开处理。

   1. **空间处理 (Spatial):**
      - 首先，像处理图片一样，将视频的**每一帧**都独立地切分成 2D 图像块（Patches）并进行嵌入。
      - 将这些 Patch Embeddings 输入一个“空间 Transformer”编码器，让模型先理解每一帧图像内部的内容。
   2. **时间处理 (Temporal):**
      - 将“空间 Transformer”的输出（例如每帧的 `[CLS]` 标记）收集起来，形成一个新的序列。这个序列代表了视频随时间变化的摘要。
      - 将这个“帧摘要”序列输入到另一个“时间 Transformer”编码器，让模型学习帧与帧之间的时序关系。

   **总结一下，无论是哪种方法，视频输入的本质都是将视频流转换成一个向量序列，这个序列中的每个向量都同时蕴含了空间信息（图像内容）和时间信息（动作变化），然后再送入 Transformer 进行处理。**
