Diffusion Policy: Visuomotor Policy Learning via Action Diffusion

### DDPM

![image-20251014231951446](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251014231951446.png)

![image-20251014231842667](C:\Users\Saki\AppData\Roaming\Typora\typora-user-images\image-20251014231842667.png)

1.  

### Diffusion Policy

1. transformer-mingpt
2. cnn- **1D U-Net**。

好的，没问题。根据我们之前详细的交流，我为您整理了一份关于这篇论文《Diffusion Policy: Visuomotor Policy Learning via Action Diffusion》1的学习笔记。笔记综合了您提出的所有问题，并以Markdown格式呈现，力求覆盖尽可能多的核心知识点。



------



# 《Diffusion Policy》学习笔记





## 一、 核心思想：它解决了什么问题？

 

**Diffusion Policy** 提出了一种全新的机器人模仿学习范式，它将生成图像的**扩散模型 (Diffusion Model)** 应用于生成机器人的**动作序列**。

- **目标**：学习一个能模仿人类专家演示的机器人策略（Policy）。

- ​    **核心方法**：将策略建模为一个**条件降噪扩散过程 (conditional denoising diffusion process)** 2。即，从一个完全随机的噪声动作序列开始，根据机器人当前的视觉观察，通过多步迭代，逐步“去噪”，最终生成一个高质量、可执行的动作序列。

  ​       




### 1.1 对比传统策略：解决了什么痛点？

为了理解Diffusion Policy的优势，需要先了解传统方法的局限：

| 策略类型                         | 工作方式                                                     | 优点                                                         | 缺点                                                         |
| -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **① 显示策略 (Explicit Policy)** | 学习一个直接映射函数 `F(观察) -> 动作`。                     | 推理速度快。                                                 | 精度差；无法解决**多模态问题**。例如，任务有两种解法时，它会学习到一个错误的“平均”动作。 |
| **② 隐式策略 (Implicit Policy)** | 学习一个**能量函数** `E(观察, 动作)`，通过优化找到能量最低的动作为输出。 | 能通过构建有多个“能量谷底”的**能量场**来优雅地解决多模态问题。 | 训练不稳定。为了推高非专家动作的能量，需要依赖**负采样**，这带来了训练波动和高计算成本 3。 |
| **③ Diffusion Policy**           | 直接学习动作分布的**梯度场**，指导噪声走向低能量（高概率）区域。 | 综合了前两者优点：能处理多模态；通过直接学习梯度场**完全避免了负采样**，训练极其稳定 4。 | 推理速度相对较慢（需要多步迭代）。                           |

------



## 二、 核心机制：扩散模型 (DDPM) 如何工作？

扩散模型包含两个过程：**前向加噪**和**反向去噪**。

### 2.1 前向过程：加噪 (Noising)

- **目的**：构造训练数据。
- **过程**：这是一个固定的数学过程，**无需网络参与**。从一个干净的数据（如动作序列） 开始，通过一个公式，可以**一步到位**地计算出任意噪声水平 下的带噪数据 。
- **公式**: !
  - ! 是从标准高斯分布中采样的纯噪声。



### 2.2 反向过程：去噪 (Denoising) - 网络的核心工作

这是模型学习和生成的核心。

#### **训练 (Training)**

- **目标**：训练一个神经网络 !，让它学会**预测噪声**。
- **过程**：
  1. 随机抽取一个干净数据 !。
  2. 随机选择**一个**时间步 !。
  3. 通过上述加噪公式生成带噪数据 !。
  4. 将 ! 和时间步 ! 送入网络 ! 中，进行**一次前向传播**，得到**预测噪声**。
  5. 计算**损失**：`Loss = MSE(预测噪声, 真实噪声 ε)`。
  6. 进行**一次反向传播**，根据Loss更新网络的所有参数。
- **关键点**：训练时，每一步都是在**随机练习**整个去噪链条中的**某一个环节**。



#### **推理 (Inference)**



- **目标**：从纯噪声中生成一个干净的数据。
- **过程**：这是一个迭代循环，**没有反向传播**。
  1. 从一个完全随机的噪声 ! 开始 (T通常为1000)。
  2. **循环k次**（从 T 到 1）：
     - 将当前的带噪数据 ! 和时间步 ! 送入**同一个**训练好的网络 !。
     - 进行**一次前向传播**，得到噪声预测。
     - 根据预测出的噪声，用去噪公式计算出更干净一点的数据 !。
  3. 最终得到 !。
- **关键点**：**“K步跑一个网络，只是跑了K次”**。我们始终只有一个参数共享的网络，它通过将时间步 ! 作为输入，来学会在不同噪声水平下执行不同的去噪策略。



### 2.3 理论基础：变分下界 (VLB)



- DDPM的优化目标是最大化数据的对数似然，但这难以直接计算。
- 实践中，我们通过最小化其**变分下界 (VLB)** 来间接实现。
- 论文作者证明，最小化复杂的VLB，等价于最小化简单的**MSE损失**，即预测噪声与真实噪声的差距。

------



## 三、 Diffusion Policy 的三大贡献



论文将标准的DDPM改造成了一个强大的机器人策略。



### 3.1 贡献一：条件化生成 (Conditional Generation)



- 标准DDPM是无条件的（如生成随机猫图）。
- Diffusion Policy加入了**条件**，即机器人的**视觉观察 !**。
- 噪声预测网络 ! 的输入变成了 `(带噪动作, 时间步, 视觉观察)`。
- 这使得去噪的每一步都受到当前环境的指导，最终生成的动作序列是符合当前场景的。



### 3.2 贡献二：后退水平控制 (Receding-Horizon Control)



- 为了让策略能实时响应环境变化，论文引入了类似**模型预测控制 (MPC)** 的思想。
- **工作循环**：
  1. **预测 (Predict)**：在当前时刻，模型一次性预测出未来一个较长时域（如16步）的完整动作序列。
  2. **执行 (Execute)**：机器人**只执行**这个序列中最开始的一小部分（如前8步）。
  3. **重复 (Repeat)**：丢弃未执行的动作，获取新的视觉观察，重新回到第1步进行新一轮的预测。
- **优点**：兼顾了动作的**长期连贯性**（长时域预测）和对干扰的**快速响应**（不断重新规划）。



### 3.3 贡献三：网络架构探索 (CNN vs. Transformer)



论文为噪声预测网络 ! 提供了两种主流架构选项。

1. **基于CNN的方案 (1D U-Net)**
   - **结构**：采用经典的**U-Net**结构，但卷积是一维的（沿着时间序列）。U-Net的“U”形编解码器结构和**跳跃连接 (Skip Connections)** 非常适合输入和输出尺寸相同的映射任务。
   - **条件融入**: 通过 **FiLM** 技术将视觉观察特征融入网络的每一层。
   - **特点**: 稳定，开箱即用。
2. **基于Transformer的方案**
   - **结构**：将动作序列视为一系列词元(tokens)，利用**自注意力机制**捕捉序列内的时间依赖关系。
   - **条件融入**: 通过 **交叉注意力 (Cross-Attention)** 机制，让动作词元动态地关注视觉观察中最相关的部分。
   - **特点**: 在需要高频、剧烈动作变化的任务上表现更好，但训练对超参数更敏感。

------



## 四、 核心概念深潜

### 4.1 能量场 vs. 梯度场

- **隐式策略**：学习一个**能量场**，即为每个动作打分的“地貌图”。能量低=动作好。这可以有多个“谷底”来表示多模态解。
- **Diffusion Policy**：不学习完整的地貌图，而是直接学习**梯度场 (Gradient Field)**。梯度场在任何一点都指向能量下降最快的方向。
  - 论文公式(8)指出，噪声预测网络 ! 近似的就是能量的负梯度 `−∇E`。
- **优势**：只学习“下山的方向”而无需构建整张地图，从而避免了估算归一化常数和负采样，使得训练极为稳定。



### 4.2 Transformer的Q, K, V机制

- **Q (Query), K (Key), V (Value)** 是注意力机制的三个核心向量。
- **工作原理比喻**:
  - **Query**: 我的问题/意图 ("我想找谁")。
  - **Key**: 别人的标签/身份 ("我是谁")。
  - **Value**: 别人实际携带的信息 ("我的内容是这个")。
- **计算流程**:
  1. 用当前词元的 **Q** 去和所有词元的 **K** 做点积，计算**相关性分数**。
  2. 用Softmax将分数归一化为**注意力权重**。
  3. 用这些权重对所有词元的 **V** 进行**加权求和**。
- **意义的涌现**: Q, K, V本身只是高维向量，没有预设含义。它们的“意义”是在海量数据的训练中，通过最小化最终任务损失，被反向传播算法“打磨”出来的。最终，`W_q`和`W_k`权重矩阵学会了将语义相关的输入投影到向量空间中更接近的位置，从而实现了我们直觉上理解的“查询-匹配”过程。

------



## 五、 代码实践与运行



- **代码结构**: 项目代码 `diffusion_policy` 5 模块化程度高，核心分为 `model`, `policy`, `dataset`, `workspace` 等目录。 

- **配置文件**: 使用 `YAML` 格式文件管理所有实验参数，灵活且易于修改。

- **运行流程**:

  1. **环境**: 通过 `conda_environment.yaml` 创建Conda环境。

  2. **数据**: 下载官方提供的演示数据集（如 `pusht_cql_v0.zip`）。

  3. **训练**: 使用 `train.py` 6 并指定 `config` 目录下的 `yaml` 文件来启动训练。

  4.  **评估**: 使用 `eval.py` 7或 `demo_pusht.py` 8 来测试和可视化训练好的模型。

     

     